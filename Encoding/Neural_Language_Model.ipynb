{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEk9_DAreFer"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoLJgt21eLMT"
   },
   "source": [
    "# Word Embeddings\n",
    "Word embeddings were proposed by  [Bengio et. al. (2001, 2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "From the dataset, the neural network would identify the words with similar meaning, but also preserve the words semantic (properties of word boy - young, male, human) and syntatic (word order, noun, grammatic relationship) propoerties.\n",
    "\n",
    "The cat is walking in the bedroom\n",
    "\n",
    "A dog was running in a room\n",
    "\n",
    "The cat is running in a room\n",
    "\n",
    "A dog is walking in a bedroom\n",
    "\n",
    "The dog was walking in the room\n",
    "\n",
    "This neural network has three components:\n",
    "\n",
    "1. An embedding layer that generates word embedding, and the parameters are shared across words.\n",
    "  * Itâ€™s a lookup table, given the index, it will return the corresponding vector.\n",
    "  * The vector representation indicated the weighted matrix is initialized as random values and will be updated by backpropagation\n",
    "2. A hidden layer of one or more layers, which introduces non-linearity to the embeddings.\n",
    "3. A softmax function that produces probability distribution over all the words in the vocabulary. \n",
    "\n",
    "The words would be represented in a dense vector\n",
    "* dog [0.2,0.5,-1.5,2.4] (length of vector is set as a parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Ul1df-h7s1"
   },
   "source": [
    "## Step 1: Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s73LQJwph3T7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WcV-OEDxj2_g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R95NfMEMkLUm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Daniel Day-Lewis is the most versatile actor a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>My guess would be this was originally going to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Well, I like to watch bad horror B-Movies, cau...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>This IS the worst movie I have ever seen, as w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I have been a Mario fan for as long as I can r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   One of the other reviewers has mentioned that ...  positive\n",
       "1   A wonderful little production. <br /><br />The...  positive\n",
       "2   I thought this was a wonderful way to spend ti...  positive\n",
       "3   Basically there's a family where a little boy ...  negative\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                ...       ...\n",
       "95  Daniel Day-Lewis is the most versatile actor a...  positive\n",
       "96  My guess would be this was originally going to...  negative\n",
       "97  Well, I like to watch bad horror B-Movies, cau...  negative\n",
       "98  This IS the worst movie I have ever seen, as w...  negative\n",
       "99  I have been a Mario fan for as long as I can r...  positive\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('C:/Users/Arnaud/Desktop/Sources/NLP-FELLOWSHIP/Data/IMDB Dataset.csv')\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHlf0dTzkVL7"
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6fci4lmbkc_q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "english_text = list(set(dataset.review)) #Depends on your data. Change code\n",
    "english_text = english_text[:5000]\n",
    "print(len(english_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "THFZyIjHkyaY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = url_pattern.sub(r'', text)\n",
    "    text = html_pattern.sub(r'', text)\n",
    "    text = re.sub(r\"[^\\w\\d'\\s]+\", ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“‚-ðŸ‰‘\n"
     ]
    }
   ],
   "source": [
    "print(u'\\U000024C2-\\U0001F251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xm4OZmDAmYI2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"series 2 has got off to a great start  i don't think you need to have watched series 1 to get a grasp of whats happening but like any series its nice to feel some sense of the characters and to care about what happens to them  and this show makes you think like that  these 4 30 something women seem to lead glamorous and exciting lives yet the premise is believable and realistic  so the twists and turns that arrive thanks to their love and sex lives are exciting to watch but you also know that these are problems that happen to real women too  its about the decisions we make as women and how sometimes we are led down certain paths in our lives rather than consciously making those choices \",\n",
       " \" a bored television director is introduced to the black arts and astral projection by his girlfriend  learning the ability to separate his spirit from his body  the man finds a renewed interest in his life and a sense of wellbeing  unfortunately  the man discovers while he is sleeping  his spirit leaves his body and his uncontrolled body roams the streets in a murderous rampage  according to the dvd sleeve's synopsis the synopsis isn't entirely correct  as it turns out anyway  the movie opens with a dizzying  out of body  example of handsome director winston rekert  as paul sharpe 's newly discovered  astral body  experience  it also foreshadows an upcoming dogfight  young andrew bednarski  as matthew sharpe  being a kid  draws pictures of  the blue man  as his murder spree begins  handsome detective john novak  as stewart kaufman  discovers the victims are connected to mr  rekert  mr  novak's investigation leads to the supernatural  a prime example of which is karen black  as janus  with whom rekert fears he is falling in love several in the cast perform well  but   the blue man  winds up tying itself up in a knot  aka  eternal evil  its unsatisfying story tries to be far too clever for its own good \",\n",
       " \"we went to the cinema expecting a biggish budget release and got an art house movie  the movie was projected digitally onto about two thirds of the screen real estate with sloping edges classic of digital projection  and had a limited stereo soundtrack which was wasted on the cinema experience the content of the film was the same old historical content we have all seen before  but heavily sanitized to prevent the audience being sick  live action scenes what little of them there were  were re used constantly in classic documentary style  which became annoying after a while i was somewhat amazed that only 4 people turned up to watch it  guess the rest knew something we didn't i suspect the producers made the film to recognize the ninetieth anniversary of gallipoli  i have to question whether they should have bothered seven out of ten for trying  and out of respect for the anzac's \",\n",
       " \"when you see this movie you begin to realise what a drastically under utilised asset the late dudley moore was  there should be a dozen movies like this in our archive he was already top notch talent before he went to hollywood  both as a comedian and a musician  but mostly he is remembered for his pairing with peter cook  on television and in one or two indifferent british movies  perhaps the best of these was 'bedazzled'  he always tended to be eclipsed by cook  who's jealousy and meanness rifted their partnership and enabled moore to realise his true potential in america  'arthur' is the result  this is a truly splendid movie  moore's clownish comedy as a drunkard is undeniable  the script is perfectly suited to his manner with lot's of hilarious  almost surreal conversational digressions  there is something so british about him that i'm actually surprised he found such an appeal to american tastes  tommy cooper  an anarchic comedian after the same fashion tended to draw a blank  it is moore's almost childish vulnerability that is so endearing liza minelli and john guilgud tend to play straight roles against him  but still have some excellent one liners  john guilgud in particular delivers his with a sarcastic and acerbic authority that is a treasure to watch  he invariably steals any scene in which he features and thoroughly deserved his oscar  correct me if i'm wrong  but he has never played any other comic role there is a follow up movie called 'arthur 2   on the rocks'  it never attains the same sublime levels of fun that this one reaches  but it is still rather good even so  guilgud only gets a cameo appearance at the beginning and as a ghost  it is darker  and there is some interesting soul searching  it will disappoint if you watch 'arthur' first hollywood seemed to loose interest in cuddly dudley after these two outings  he eventually returned to britain  dejected and apparently dying but 'arthur' is a sample of what might have been  we can only imagine the other great movies he should have made your're sadly missed  dudley \",\n",
       " \"if you never have read the book and never intend to read it in the future  go on and watch the movie  6 10  it is a nice fantasy movie with well done cgi  nice acting  a beautiful environment and an above average fantasy story if you have read the book like me about 10 times or more and really love it  don't expect too much  or better  don't expect anything at all  the story is totally different from the original book  this may explain that the movie is voted 1 10 from people around 40 or more  like me  and much better from people who most probably never read the book before and thus expect nothing most of the differences between movie and book are not really necessary and change the setting  in my opinion much to the worse    the magic in the book works with rituals for classic magical effects   changing weather  creating illusions  transform into animals    in the movie the magic is more like  jedi school for the middle ages   tm   wooden sticks instead of lightsabers  that the devil is looking like emperor palpatine  after part iii  doesn't make it really better  the mill in the book is not totally cut off the world like in the movie  in the book the story is set near dresden  which krabat visits one time with his master and also he visits some nearby villages for festivities   this part might have been changed to cut costs  i also don't understand why in the movie the mill is located in the hills while the nearby graveyard is set in the high mountains  the whole surrounding is the average run of the mill fantasy medieval style  lots of mud everywhere  dirty faces  not an orderly kitchen  only very rough houses  the book never suggested such an environment  in the book the master tries to make krabat his successor but krabat rejects  krabat is somewhere between admiration  distance and silent rejection  in the movie krabat rejects the master always openly like a stubborn schoolboy  the movie is set in 1647 instead of around 1720  this makes it impossible for the master to tell some stories from his youth probably around 170x  ok  the stories are missing anyway in the movie also some explanations given in the book would have been helpful and would not cost so much minutes    in the book all work done at day is effortless and work in the night is like normal work  this explanation is missing in the movie  sometimes the boys are sweating and sometimes they are happy  the book explains why only a few  gesellen  try to confront the master  if the master dies by any mundane reasons  the  gesellen  are free and keep their magical powers  if the master dies at the confrontation  all will lose their power forever \"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_english_text = []\n",
    "for sent in english_text:\n",
    "  processed_english_text.append(preprocess(sent))\n",
    "\n",
    "print(len(processed_english_text))\n",
    "processed_english_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "75FIiw93Dnzx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "UNK_symbol = \"<UNK>\"\n",
    "all_words = set([UNK_symbol])\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSGYg8lXn6a7"
   },
   "source": [
    "## Step 2: index the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5is9OW_emoiJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43211 unique words\n"
     ]
    }
   ],
   "source": [
    "UNK_symbol = \"<UNK>\"\n",
    "all_words = set([UNK_symbol]) #split and put items into a set\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for sentence in processed_english_text:\n",
    "  words = [token for token in sentence.split() ] #tokenisation\n",
    "  all_words.update(words) #create a list of unique words\n",
    "\n",
    "for index,value in enumerate(all_words):\n",
    "  word_to_index[value] = index\n",
    "  index_to_word[index] = value\n",
    "  \n",
    "n_class = len(word_to_index) # number of Vocabulary\n",
    "\n",
    "print(n_class, 'unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AjvkMeP7pKRV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keeper',\n",
       " 'unites',\n",
       " 'copycats',\n",
       " 'weaver',\n",
       " 'scripters',\n",
       " 'itself',\n",
       " 'tamblyn',\n",
       " 'haven',\n",
       " 'calves',\n",
       " \"f'n\",\n",
       " 'burping',\n",
       " 'nxd',\n",
       " 'kudos',\n",
       " 'mushy',\n",
       " 'triumph',\n",
       " 'ridgemont',\n",
       " 'tending',\n",
       " 'eamon',\n",
       " 'belief',\n",
       " 'debi',\n",
       " \"'yes'\",\n",
       " 'numerable',\n",
       " 'storywriting',\n",
       " 'queue',\n",
       " 'pheri',\n",
       " 'recall',\n",
       " 'complacent',\n",
       " 'soundtracks',\n",
       " 'muscular',\n",
       " 'nurturing']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_to_index, key=word_to_index.get)[10:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVx10PzPqdC2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KubfKJMfwboX"
   },
   "outputs": [],
   "source": [
    "def create_input_target(list_sentences):\n",
    "  input_batch = []\n",
    "  target_batch = []\n",
    "\n",
    "  for sen in list_sentences:\n",
    "      word = sen.split() # space tokenizer\n",
    "      input = [word_to_index[n] for n in word[:-1]] # create (1~n-1) as input (every word in the sentence except the last word)\n",
    "      target = word_to_index[word[-1]] # create (n) as target, We usually call this 'casual language model'\n",
    "\n",
    "      input_batch.append(torch.tensor(input))\n",
    "      target_batch.append(target)\n",
    "\n",
    "  return input_batch,target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riGKl5XyyfIu"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWeANuWp0SRT"
   },
   "outputs": [],
   "source": [
    "#Use this cell for testing the shape of input when passed through different layers\n",
    "# X =nn.Embedding(n_class, 2)(input_batch)\n",
    "# X=X.view(-1, 2 * 2)\n",
    "# #tanh = torch.tanh(nn.Parameter(torch.ones(2)) + nn.Linear(2 * 2, 2, bias=False)(X))\n",
    "# nn.Parameter(torch.ones(2))\n",
    "# #nn.Linear(2 * 2, 2, bias=False)\n",
    "# # # nn.Linear(2, n_class, bias=False)\n",
    "# # # nn.Linear(2 * 2, n_class, bias=False)\n",
    "# # # nn.Parameter(torch.ones(n_class))\n",
    "# # #torch.LongTensor(input_batch[0])\n",
    "# #tanh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B93Ep3vxyit0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_class, m)\n",
    "        self.hidden1 = nn.Linear(n_step * m, n_hidden, bias=False)\n",
    "        self.ones = nn.Parameter(torch.ones(n_hidden))\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.hidden3 = nn.Linear(n_step * m, n_class, bias=False)\n",
    "        self.bias = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embeddings(X) # X : [batch_size, n_step, m]\n",
    "        \n",
    "        #X = X.flatten()\n",
    "        X = X.view(-1, n_step * m) # [batch_size, n_step * m] first layer (-1 flattens the tensor)\n",
    "        \n",
    "        tanh = torch.tanh(self.ones + self.hidden1(X)) # [batch_size, n_hidden] pass embedded layer through first hidden layet and add bias. The result is passed through tanh function\n",
    "        \n",
    "        output = self.bias + self.hidden3(X) + self.hidden2(tanh) # [batch_size, n_class]\n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  n_step = 2 # number of steps, n-1 in paper\n",
    "  n_hidden = 2 # number of hidden size, h in paper\n",
    "  m = 10 # embedding size, m in paper,( batch size)\n",
    "  \n",
    "  gpu = 0\n",
    "  input_batch,target_batch = create_input_target(processed_english_text)\n",
    "  input_batch = torch.LongTensor(pad_sequence(input_batch))\n",
    "  train_loader = DataLoader(input_batch, batch_size = 10, num_workers = 1)\n",
    "  \n",
    "  target_batch = torch.LongTensor(target_batch)\n",
    "  dev_loader = DataLoader(target_batch, batch_size = 10, num_workers = 1)\n",
    "  \n",
    "\n",
    "  model = NNLM()\n",
    "  model.cuda(gpu)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lX1vr5mfOQTj"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(50):\n",
    "    st = time.time()\n",
    "    \n",
    "    for it, data_tensor in enumerate(train_loader):  \n",
    "      \n",
    "      context_tensor = data_tensor[:,0:2]\n",
    "      target_tensor = data_tensor[:,2]\n",
    "\n",
    "      context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      output = model(context_tensor)\n",
    "\n",
    "      #acc = get_accuracy_from_log_probs(output, target_tensor)\n",
    "\n",
    "      # output : [batch_size, n_class], target_batch : [batch_size]\n",
    "      loss = criterion(output, target_tensor)\n",
    "      if (epoch + 1) % 5 == 0:\n",
    "          print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss),'output size: ',output.size)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if it % 500 == 0: \n",
    "        print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
    "        st = time.time()\n",
    "\n",
    "      \n",
    "      # set best model path\n",
    "      best_model_path = 'model/best_model_{}.dat'.format(epoch)\n",
    "      # saving best model\n",
    "      torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV7XBPqXPWDh",
    "outputId": "9848cc59-c29e-4a1f-9738-a13a5b014536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('london', 'paris'): -0.20533907413482666, ('male', 'husband'): 0.5646224617958069, ('women', 'wife'): 0.06242431700229645, ('king', 'man'): -0.07820046693086624}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = NNLM()\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.cuda(gpu)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "lm_similarities = {}\n",
    "# word pairs to calculate similarity\n",
    "words = {('women','wife'),('king','man'),('london','paris'),('male','husband')}\n",
    "\n",
    "# ----------- Calculate LM similarities using cosine similarity ----------\n",
    "for word_pairs in words:\n",
    "    w1 = word_pairs[0]\n",
    "    w2 = word_pairs[1]\n",
    "    words_tensor = torch.LongTensor([word_to_index.get(w1,word_to_index['<UNK>']),word_to_index.get(w2,word_to_index['<UNK>'])])\n",
    "    \n",
    "    words_tensor = words_tensor.cuda(gpu)\n",
    "    # get word embeddings from the best model\n",
    "    words_embeds = best_model.embeddings(words_tensor)\n",
    "    # calculate cosine similarity between word vectors\n",
    "    sim = cos(words_embeds[0],words_embeds[1])\n",
    "    lm_similarities[word_pairs] = sim.item()\n",
    "\n",
    "print(lm_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdLkuEfybHd-",
    "outputId": "09db4aa6-f8d0-4cd4-e97e-46cc9116e6ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7643,  1.5840, -1.5441, -1.8176, -0.9911,  0.4807,  0.5916,  0.4323,\n",
       "          1.4739, -0.6284]], device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.embeddings(torch.LongTensor([word_to_index.get('london')]).cuda(gpu)) # Test the embeddings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF1nperYrxHW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
